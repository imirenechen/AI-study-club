{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imirenechen/AI-study-club/blob/main/01_unet_cars_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PaxoqFkMAVhJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "743cbce0-4064-4c9b-ad6f-5df9ac1f5f20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks/Cars\n"
          ]
        }
      ],
      "source": [
        "# Mount to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Move to your current working directory\n",
        "%cd drive/MyDrive/Colab\\ Notebooks/Cars"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all the packages\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "s3gL3LFHAed8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build one of the main components - DoubleConv - for UNet\n",
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "      nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "      nn.ReLU(inplace=True),\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)"
      ],
      "metadata": {
        "id": "OQospNIGB9CM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build UNet from scrach\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
        "    super().__init__()\n",
        "    self.downs = nn.ModuleList()\n",
        "    for feature in features:\n",
        "      self.downs.append(DoubleConv(in_channels, feature))\n",
        "      in_channels = feature\n",
        "    self.bottlenect = DoubleConv(features[-1], features[-1]*2)\n",
        "    self.ups = nn.ModuleList()\n",
        "    for feature in reversed(features):\n",
        "      self.ups.append(nn.ConvTranspose2d(feature*2, feature, 2, 2))\n",
        "      self.ups.append(DoubleConv(feature*2, feature))\n",
        "    self.final_conv = nn.Conv2d(features[0], out_channels, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    skip_connections = []\n",
        "    for down in self.downs:\n",
        "      x = down(x)\n",
        "      skip_connections.append(x)\n",
        "      x = F.max_pool2d(x, (2, 2))\n",
        "    x = self.bottlenect(x)\n",
        "    skip_connections.reverse() # inplace\n",
        "    for i in range(0, len(self.ups), 2):\n",
        "      x = self.ups[i](x)\n",
        "      skip_connection = skip_connections[i//2]\n",
        "      concat = torch.cat((skip_connection, x), dim=1) # N x C x H x W\n",
        "      x = self.ups[i+1](concat)\n",
        "    return self.final_conv(x)"
      ],
      "metadata": {
        "id": "rZSFGD-sCHtV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model only\n",
        "model = UNet()\n",
        "toy_data = torch.ones([16, 3, 240, 160])\n",
        "out = model(toy_data)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF1qK44Q6oHy",
        "outputId": "013ca870-8932-472a-a117-a001559e6282"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 240, 160])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an UNet model object\n",
        "model = UNet()\n",
        "\n",
        "# Move the model to GPU\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "MO5Xg0pUJbNp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build CustomDataset for loading data from Google Drive\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, image_dir, mask_dir, transform):\n",
        "    super().__init__()\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "    self.images = os.listdir(self.image_dir)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    image_path = os.path.join(self.image_dir, self.images[i])\n",
        "    mask_path = os.path.join(self.mask_dir, self.images[i].replace(\".jpg\", \"_mask.gif\"))\n",
        "    image = np.array(Image.open(image_path))\n",
        "    mask = np.array(Image.open(mask_path).convert(\"L\"))\n",
        "    return self.transform(image), self.transform(mask)"
      ],
      "metadata": {
        "id": "ILgSgfqFJidq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the device we are using is GPU or CPU\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "5eqoqZEaHUzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0ed9f71-7928-44e6-cfd3-27bd89077a6e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants for UNet model training process\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 3\n",
        "IMG_WIDTH = 240\n",
        "IMG_HEIGHT = 160"
      ],
      "metadata": {
        "id": "sBoa09DRHUtm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "all_data = CustomDataset('small_train', 'small_train_masks', T.Compose([T.ToTensor(), T.Resize((IMG_HEIGHT, IMG_WIDTH))]))"
      ],
      "metadata": {
        "id": "rd67NulqHUly"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and val\n",
        "train_data, val_data = torch.utils.data.random_split(all_data, [0.7, 0.3])"
      ],
      "metadata": {
        "id": "GwM6Vz5NKtfm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create loader for mini-batch gradient descent\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "Gp2ZXGzHLYmt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The loss function for bianry classification\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "# Choosing Adam as our optimizer\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "jEJ-RbO6UzJP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, num_epochs, train_loader, optimizer, print_every=30):\n",
        "  for epoch in range(num_epochs):\n",
        "    for count, (x, y) in enumerate(train_loader):\n",
        "      model.train()\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      out = model(x)\n",
        "      if count % print_every == 0:\n",
        "        eval(model, val_loader, epoch)\n",
        "      out = torch.sigmoid(out)\n",
        "      loss = loss_function(out, y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()"
      ],
      "metadata": {
        "id": "fYS30O-dSl0L"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model, val_loader, epoch):\n",
        "  model.eval()\n",
        "  num_correct = 0\n",
        "  num_pixels = 0\n",
        "  with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      out_img = model(x)\n",
        "      probability = torch.sigmoid(out_img)\n",
        "      predictions = probability>0.5\n",
        "      num_correct += (predictions==y).sum()\n",
        "      num_pixels += BATCH_SIZE*IMG_WIDTH*IMG_HEIGHT\n",
        "  print(f'Epoch[{epoch+1}] Acc: {num_correct/num_pixels}')\n"
      ],
      "metadata": {
        "id": "4weW5Wi8RWMW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, NUM_EPOCHS, train_loader, optimizer)"
      ],
      "metadata": {
        "id": "8rcL1usEWTHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dff29c73-7449-4670-f98a-5c0bf5f7daab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[1] Acc: 0.790369987487793\n",
            "Epoch[1] Acc: 0.8479331731796265\n",
            "Epoch[1] Acc: 0.9744017124176025\n",
            "Epoch[2] Acc: 0.9809433817863464\n",
            "Epoch[2] Acc: 0.9876761436462402\n",
            "Epoch[2] Acc: 0.9887269139289856\n",
            "Epoch[3] Acc: 0.9901547431945801\n",
            "Epoch[3] Acc: 0.9917691946029663\n",
            "Epoch[3] Acc: 0.9908760786056519\n"
          ]
        }
      ]
    }
  ]
}